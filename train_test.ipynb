{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sp20zcF8ZPDS",
    "outputId": "bf8a8bf4-62b1-4ff7-b864-404a1f0c81a1",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# required modules\n",
    "\n",
    "!pip install --upgrade wandb==0.22.3   # needed for logging\n",
    "!git clone https://github.com/Verified-Intelligence/auto_LiRPA\n",
    "!pip install ./auto_LiRPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2026-01-29T15:42:51.243126Z",
     "iopub.status.busy": "2026-01-29T15:42:51.242847Z",
     "iopub.status.idle": "2026-01-29T15:42:58.558976Z",
     "shell.execute_reply": "2026-01-29T15:42:58.558360Z",
     "shell.execute_reply.started": "2026-01-29T15:42:51.243087Z"
    },
    "id": "crJQBHZtZPDV",
    "outputId": "7353e746-da13-40ec-d42f-1f24746f0dda",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# used for logging\n",
    "import wandb\n",
    "\n",
    "wandb_key = ... # YOUR KEY\n",
    "wandb.login(key=wandb_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4a2GohC8ZPDW"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Let's import the needed modules and load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T15:42:58.560647Z",
     "iopub.status.busy": "2026-01-29T15:42:58.560184Z",
     "iopub.status.idle": "2026-01-29T15:43:09.210628Z",
     "shell.execute_reply": "2026-01-29T15:43:09.209839Z",
     "shell.execute_reply.started": "2026-01-29T15:42:58.560622Z"
    },
    "id": "kzooQDi8Y5LW",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from typing import Callable, Tuple\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from auto_LiRPA import BoundedModule, BoundedTensor, PerturbationLpNorm\n",
    "\n",
    "from model import CNNCrown, Encoder, LinearClassifier\n",
    "from losses import SupConLoss\n",
    "from verifier import PGDVerifier\n",
    "from utils import train, test, get_device, get_embeddings_plot, RandomGaussianNoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2026-01-29T15:43:09.212475Z",
     "iopub.status.busy": "2026-01-29T15:43:09.211794Z",
     "iopub.status.idle": "2026-01-29T15:43:09.246797Z",
     "shell.execute_reply": "2026-01-29T15:43:09.246104Z",
     "shell.execute_reply.started": "2026-01-29T15:43:09.212417Z"
    },
    "id": "dPGo5FDrZb15",
    "outputId": "ecc6a898-427a-4dd5-a2f6-0d5554319222",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DEVICE = get_device()\n",
    "BATCH_SIZE = 128\n",
    "PROJ_DIM = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2026-01-29T15:45:52.257233Z",
     "iopub.status.busy": "2026-01-29T15:45:52.256895Z",
     "iopub.status.idle": "2026-01-29T15:45:53.873673Z",
     "shell.execute_reply": "2026-01-29T15:45:53.873075Z",
     "shell.execute_reply.started": "2026-01-29T15:45:52.257200Z"
    },
    "id": "RfIHtNRrZdFE",
    "outputId": "548e5d2e-1a57-4a15-9041-5d56f5d99cd4",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "AUGMENTATION = False\n",
    "AUGMENTATION_LABEL = \"\" if AUGMENTATION else \"No\"\n",
    "\n",
    "if AUGMENTATION:\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ColorJitter(0.4, 0.4, 0.4, 0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        RandomGaussianNoise(p=0.5),\n",
    "    ])\n",
    "else:\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    "\n",
    "torch.manual_seed(42)\n",
    "dataset = datasets.CIFAR10(root=\"data\", train=True, download=True, transform=transform)\n",
    "\n",
    "train_ratio, validation_ratio = 0.8, 0.2\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(train_ratio * dataset_size)\n",
    "validation_size = dataset_size - train_size\n",
    "\n",
    "train_dataset, validation_dataset = random_split(dataset, [train_size, validation_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root=\"data\", train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrastive Model Training\n",
    "\n",
    "Steps needed:\n",
    "1. train the encoder with supervised contrastive loss;\n",
    "2. train the classifier with cross entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rovSzx1vqBpN",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Encoder Training\n",
    "EPOCHS = 20\n",
    "learning_rate = 1e-3\n",
    "sup_con_loss = SupConLoss()\n",
    "encoder = Encoder(proj_dim=PROJ_DIM).to(DEVICE)\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wbMbYbmpZPDX",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"Cnn-Verification\",\n",
    "    name=f\"Encoder - {AUGMENTATION_LABEL} Augmentation\",\n",
    "    config={\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"projection_dimension\": PROJ_DIM,\n",
    "        \"loss\": \"Supervised Contrastive Loss\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W8_xdaybqBpN",
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "encoder = train(\n",
    "    encoder,\n",
    "    train_loader,\n",
    "    validation_loader,\n",
    "    encoder_optimizer,\n",
    "    sup_con_loss,\n",
    "    EPOCHS,\n",
    "    DEVICE,\n",
    "    compute_accuracy=False,\n",
    "    wandb_logging=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C0vcekNgZPDY",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# logging embeddings\n",
    "buf = get_embeddings_plot(encoder, train_loader, validation_loader, DEVICE)\n",
    "wandb.log({\"embeddings_space\": wandb.Image(Image.open(buf))})\n",
    "# logging weights\n",
    "model_filename = \"encoder_weights.pt\"\n",
    "torch.save(encoder.state_dict(), model_filename)\n",
    "artifact = wandb.Artifact(\"model\", type=\"model\")\n",
    "artifact.add_file(f\"/kaggle/working/{model_filename}\")\n",
    "wandb.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IJ-AQKgWZPDY",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l8sLsSnBz3GF",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Classifier Training\n",
    "EPOCHS = 10\n",
    "learning_rate = 0.001\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "classifier = LinearClassifier(in_dim=PROJ_DIM).to(DEVICE)\n",
    "classifier_optimizer = optim.Adam(classifier.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I7ZpD-TqZPDY",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"Cnn-Verification\",\n",
    "    name=f\"Classifier - {AUGMENTATION_LABEL} Augmentation\",\n",
    "    config={\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"projection_dimension\": PROJ_DIM,\n",
    "        \"loss\": \"CrossEntropyLoss\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "93cjIpe5qBpN",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def execute_classifier(encoder:nn.Module) -> Callable[[torch.Tensor, torch.Tensor], tuple[torch.Tensor, torch.Tensor]]:\n",
    "    # used to compute the embeddings given the encoder\n",
    "    \n",
    "    def main(images:torch.Tensor, labels:torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        encoder.eval()\n",
    "        with torch.no_grad():\n",
    "            embeddings = encoder(images)\n",
    "        return embeddings, labels\n",
    "    return main\n",
    "\n",
    "classifier = train(\n",
    "    classifier,\n",
    "    train_loader,\n",
    "    validation_loader,\n",
    "    classifier_optimizer,\n",
    "    cross_entropy_loss,\n",
    "    EPOCHS,\n",
    "    DEVICE,\n",
    "    middleware=execute_classifier(encoder),\n",
    "    wandb_logging=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z0zOIgsiZPDY",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# logging weights\n",
    "model_filename = \"classifier_weights.pt\"\n",
    "torch.save(classifier.state_dict(), model_filename)\n",
    "artifact = wandb.Artifact(\"model\", type=\"model\")\n",
    "artifact.add_file(f\"/kaggle/working/{model_filename}\")\n",
    "wandb.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aKT81dKBZPDY",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Model Training\n",
    "\n",
    "Training together the encoder and classifier using cross entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zNjr_UC7ZPDY",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Normal Model Training\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 128\n",
    "learning_rate = 0.001\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "full_model = CNNCrown(proj_dim=PROJ_DIM).to(DEVICE)\n",
    "full_model_optimizer = optim.Adam(full_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yMaNHuGWZPDY",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"Cnn-Verification\",\n",
    "    name=f\"Normal Model - {AUGMENTATION_LABEL} Augmentation\",\n",
    "    config={\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"projection_dimension\": PROJ_DIM,\n",
    "        \"loss\": \"CrossEntropy\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qJGHyjo6qBpN",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "full_model = train(\n",
    "    full_model,\n",
    "    train_loader,\n",
    "    validation_loader,\n",
    "    full_model_optimizer,\n",
    "    cross_entropy_loss,\n",
    "    EPOCHS,\n",
    "    DEVICE,\n",
    "    wandb_logging=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QHicNVnCZPDZ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# logging embeddings\n",
    "buf = get_embeddings_plot(full_model.encoder, train_loader, validation_loader, DEVICE)\n",
    "wandb.log({\"embeddings_space\": wandb.Image(Image.open(buf))})\n",
    "# logging weights\n",
    "model_filename = \"full_model_weights.pt\"\n",
    "torch.save(full_model.state_dict(), model_filename)\n",
    "artifact = wandb.Artifact(\"model\", type=\"model\")\n",
    "artifact.add_file(f\"/kaggle/working/{model_filename}\")\n",
    "wandb.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WQX-I54lZPDZ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Model with Adversarial Training\n",
    "\n",
    "Training together the encoder and the classifier using cross entropy loss. During the training the batch is enlarged with adversarial examples found using PGD.\n",
    "\n",
    "**Important**: the batch size you set at the beginning is going to be doubled because of the adversarial examples. Halve it if needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nhS7eEp9ZPDZ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Adversarial Training\n",
    "\n",
    "EPOCHS = 10\n",
    "learning_rate = 0.001\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "adversarial_model = CNNCrown(proj_dim=PROJ_DIM).to(DEVICE)\n",
    "adversarial_model_optimizer = optim.Adam(adversarial_model.parameters(), lr=learning_rate)\n",
    "pgd = PGDVerifier(device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nY0OQv77ZPDZ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"Cnn-Verification\",\n",
    "    name=f\"Adversarial Model - {AUGMENTATION_LABEL} Augmentation\",\n",
    "    config={\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE * 2,\n",
    "        \"projection_dimension\": PROJ_DIM,\n",
    "        \"loss\": \"CrossEntropy\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J9JOvCSYZPDZ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_adversarial_examples(adversarial_model:nn.Module) -> Callable[[torch.Tensor, torch.Tensor], tuple[torch.Tensor, torch.Tensor]]:\n",
    "    # used to enlarge the bacth size with the adversarial examples\n",
    "\n",
    "    def main(images:torch.Tensor, labels:torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        adversarial_examples, _, _ = pgd.verify(adversarial_model, images, labels, clamp_min=-1, clamp_max=1)\n",
    "        adversarial_examples.requires_grad = False\n",
    "        images = torch.cat([images, adversarial_examples])\n",
    "        labels = torch.cat([labels, labels])\n",
    "        return images, labels\n",
    "    return main\n",
    "\n",
    "adversarial_model = train(\n",
    "    adversarial_model,\n",
    "    train_loader,\n",
    "    validation_loader,\n",
    "    adversarial_model_optimizer,\n",
    "    cross_entropy_loss,\n",
    "    EPOCHS,\n",
    "    DEVICE,\n",
    "    middleware=compute_adversarial_examples(adversarial_model),\n",
    "    wandb_logging=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zXHVW6MmZPDZ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# logging embeddings\n",
    "buf = get_embeddings_plot(adversarial_model.encoder, train_loader, validation_loader, DEVICE)\n",
    "wandb.log({\"embeddings_space\": wandb.Image(Image.open(buf))})\n",
    "# logging weights\n",
    "model_filename = \"adversarial_model_weights.pt\"\n",
    "torch.save(adversarial_model.state_dict(), model_filename)\n",
    "artifact = wandb.Artifact(\"model\", type=\"model\")\n",
    "artifact.add_file(f\"/kaggle/working/{model_filename}\")\n",
    "wandb.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tvgeshNDZPDZ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrastive Model with Adversarial Triaining\n",
    "Steps:\n",
    "1. enlarge the batch using adversarial examples;\n",
    "2. train the encoder with supervised contrastive loss;\n",
    "3. train the classifier with cross entropy loss.\n",
    "\n",
    "**Important**: the batch size defined above is going to be doubled because of adversarial examples. Halve it if needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CgiL0MIWZPDZ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Adversarial Training With Supervised Contrastive Loss\n",
    "\n",
    "# --- Encoder Training\n",
    "EPOCHS = 40\n",
    "learning_rate = 1e-3\n",
    "sup_con_loss = SupConLoss(temperature=0.1)\n",
    "adversarial_encoder = Encoder(proj_dim=PROJ_DIM).to(DEVICE)\n",
    "adversarial_encoder_optimizer = optim.Adam(adversarial_encoder.parameters(), lr=learning_rate)\n",
    "pgd = PGDVerifier(device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aYp49jF4ZPDa",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"Cnn-Verification\",\n",
    "    name=f\"Adversarial Contrastive Encoder - {AUGMENTATION_LABEL} Augmentation\",\n",
    "    config={\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE * 2,\n",
    "        \"projection_dimension\": PROJ_DIM,\n",
    "        \"loss\": \"Supervised Contrastive Loss\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7vWb8Mm6ZPDa",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_adversarial_examples(adversarial_encoder:nn.Module) -> Callable[[torch.Tensor, torch.Tensor], tuple[torch.Tensor, torch.Tensor]]:\n",
    "    # used to add the adversarial examples in the batch\n",
    "    sup_con_loss = SupConLoss()\n",
    "    \n",
    "    def main(images:torch.Tensor, labels:torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        adversarial_examples, _, _ = pgd.verify(adversarial_encoder, images, labels, clamp_min=-1, clamp_max=1, criterion=sup_con_loss)\n",
    "        adversarial_examples.requires_grad = False\n",
    "        images = torch.cat([images, adversarial_examples])\n",
    "        labels = torch.cat([labels, labels])\n",
    "        return images, labels\n",
    "\n",
    "    return main\n",
    "\n",
    "adversarial_encoder = train(\n",
    "    adversarial_encoder,\n",
    "    train_loader,\n",
    "    validation_loader,\n",
    "    adversarial_encoder_optimizer,\n",
    "    sup_con_loss,\n",
    "    EPOCHS,\n",
    "    DEVICE,\n",
    "    middleware=compute_adversarial_examples(adversarial_encoder),\n",
    "    compute_accuracy=False,\n",
    "    wandb_logging=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E-6Ox7NYZPDa",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# logging embeddings\n",
    "buf = get_embeddings_plot(adversarial_encoder, train_loader, validation_loader, DEVICE)\n",
    "wandb.log({\"embeddings_space\": wandb.Image(Image.open(buf))})\n",
    "# logging weights\n",
    "model_filename = \"adversarial_encoder_weights.pt\"\n",
    "torch.save(adversarial_encoder.state_dict(), model_filename)\n",
    "artifact = wandb.Artifact(\"model\", type=\"model\")\n",
    "artifact.add_file(f\"/kaggle/working/{model_filename}\")\n",
    "wandb.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IVNae68TZPDa",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H_TNSGJeZPDa",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training the Linear Classifier\n",
    "\n",
    "EPOCHS = 10\n",
    "learning_rate = 0.001\n",
    "adversarial_encoder.eval()\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "adversarial_classifier = LinearClassifier(in_dim=PROJ_DIM).to(DEVICE)\n",
    "adversarial_classifier_optimizer = optim.Adam(adversarial_classifier.parameters(), lr=learning_rate)\n",
    "pgd = PGDVerifier(device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99PilRbhZPDa",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"Cnn-Verification\",\n",
    "    name=f\"Adversarial Contrastive Classifier - {AUGMENTATION_LABEL} Augmentation\",\n",
    "    config={\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE * 2,\n",
    "        \"projection_dimension\": PROJ_DIM,\n",
    "        \"loss\": \"CrossEntropy Loss\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q6npZsiHZPDa",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_adversarial_examples(adversarial_encoder:nn.Module) -> Callable[[torch.Tensor, torch.Tensor], tuple[torch.Tensor, torch.Tensor]]:\n",
    "    # used to compute the embeddigns with the encoder and to add the adversarial examples in the batch\n",
    "    \n",
    "    def main(images:torch.Tensor, labels:torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        adversarial_examples, _, _ = pgd.verify(adversarial_encoder, images, labels, clamp_min=-1, clamp_max=1)\n",
    "        adversarial_examples.requires_grad = False\n",
    "        images = torch.cat([images, adversarial_examples])\n",
    "        labels = torch.cat([labels, labels])\n",
    "        with torch.no_grad():\n",
    "            embeddings = adversarial_encoder(images)\n",
    "        return embeddings, labels\n",
    "\n",
    "    return main\n",
    "\n",
    "adversarial_classifier = train(\n",
    "    adversarial_classifier,\n",
    "    train_loader,\n",
    "    validation_loader,\n",
    "    adversarial_classifier_optimizer,\n",
    "    cross_entropy_loss,\n",
    "    EPOCHS,\n",
    "    DEVICE,\n",
    "    middleware=compute_adversarial_examples(adversarial_encoder),\n",
    "    wandb_logging=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J6XnjJIiZPDb",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# logging weights\n",
    "model_filename = \"adversarial_classifier_weights.pt\"\n",
    "torch.save(adversarial_classifier.state_dict(), model_filename)\n",
    "artifact = wandb.Artifact(\"model\", type=\"model\")\n",
    "artifact.add_file(f\"/kaggle/working/{model_filename}\")\n",
    "wandb.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1O6FpLfMZPDb",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Certified Model\n",
    "\n",
    "Training together the encoder and classifier with cross entropy loss. The input is perturbed by an $\\epsilon$ value, relative bounds are propagated using CROWN-IBP and the lower bound is passed to the loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T15:46:00.905558Z",
     "iopub.status.busy": "2026-01-29T15:46:00.904839Z",
     "iopub.status.idle": "2026-01-29T15:46:00.910128Z",
     "shell.execute_reply": "2026-01-29T15:46:00.909417Z",
     "shell.execute_reply.started": "2026-01-29T15:46:00.905528Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "epsilon = 2/255  # image perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2026-01-29T15:46:01.425979Z",
     "iopub.status.busy": "2026-01-29T15:46:01.425151Z",
     "iopub.status.idle": "2026-01-29T15:46:01.490327Z",
     "shell.execute_reply": "2026-01-29T15:46:01.489675Z",
     "shell.execute_reply.started": "2026-01-29T15:46:01.425950Z"
    },
    "id": "scq_uiUkfLEP",
    "outputId": "858c023b-ff89-4201-974d-4065d72f6fbb",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Certified Model Train\n",
    "\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 128\n",
    "learning_rate = 0.001\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "certified_model = CNNCrown(proj_dim=PROJ_DIM).to(DEVICE)\n",
    "certified_model = BoundedModule(certified_model, torch.empty(2, 3, 32, 32))\n",
    "certified_model_optimizer = optim.Adam(certified_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "execution": {
     "iopub.execute_input": "2026-01-29T15:45:24.279536Z",
     "iopub.status.busy": "2026-01-29T15:45:24.278868Z",
     "iopub.status.idle": "2026-01-29T15:45:30.781236Z",
     "shell.execute_reply": "2026-01-29T15:45:30.780626Z",
     "shell.execute_reply.started": "2026-01-29T15:45:24.279502Z"
    },
    "id": "Thb4IFZ2f0pg",
    "outputId": "57106018-3350-4a22-be7d-d7ad86bc5f6f",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"Cnn-Verification\",\n",
    "    name=f\"Certified Model - {AUGMENTATION_LABEL} Augmentation\",\n",
    "    config={\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE * 2,\n",
    "        \"projection_dimension\": PROJ_DIM,\n",
    "        \"loss\": \"CrossEntropy Loss\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2026-01-29T15:46:04.671789Z",
     "iopub.status.busy": "2026-01-29T15:46:04.671436Z",
     "iopub.status.idle": "2026-01-29T16:02:36.539673Z",
     "shell.execute_reply": "2026-01-29T16:02:36.538886Z",
     "shell.execute_reply.started": "2026-01-29T15:46:04.671762Z"
    },
    "id": "X3WPKo34fQ5Z",
    "outputId": "bc605328-c476-4cdf-c7cf-9cf5bb6caae9",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train loop\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=ResourceWarning)\n",
    "\n",
    "ptb = PerturbationLpNorm(norm=float('inf'), eps=epsilon)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    certified_model.train()\n",
    "    train_loss = 0\n",
    "    train_accuracy = 0\n",
    "    for _, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        images_bounded = BoundedTensor(images, ptb)\n",
    "\n",
    "        certified_model_optimizer.zero_grad()\n",
    "        lb, ub = certified_model.compute_bounds(x=(images_bounded,), method=\"CROWN-IBP\")\n",
    "        loss = cross_entropy_loss(lb, labels)\n",
    "        loss.backward()\n",
    "        certified_model_optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_accuracy += (torch.argmax(lb, dim=1) == labels).sum().item() / len(labels)\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_accuracy = train_accuracy / len(train_loader) * 100\n",
    "\n",
    "    certified_model.eval()\n",
    "    with torch.no_grad():\n",
    "        validation_loss = 0\n",
    "        validation_accuracy = 0\n",
    "        for images, labels in validation_loader:\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            images_bounded = BoundedTensor(images, ptb)\n",
    "\n",
    "            lb, ub = certified_model.compute_bounds(x=(images_bounded,), method=\"CROWN-IBP\")\n",
    "            loss = cross_entropy_loss(lb, labels)\n",
    "\n",
    "            validation_loss += loss.item()\n",
    "            validation_accuracy += (torch.argmax(lb, dim=1) == labels).sum().item() / len(labels)\n",
    "\n",
    "    validation_loss /= len(validation_loader)\n",
    "    validation_accuracy = validation_accuracy / len(validation_loader) * 100\n",
    "\n",
    "    print(f\"> Epoch {epoch+1}/{EPOCHS}\")\n",
    "    print(f\"  Training loss      : {train_loss:.4f}, Training accuracy  : {train_accuracy:.2f}%\")\n",
    "    print(f\"  Validation loss    : {validation_loss:.4f}, Validation accuracy: {validation_accuracy:.2f}%\")\n",
    "\n",
    "    log = {\n",
    "        \"train_loss\": train_loss,\n",
    "        \"validation_loss\": validation_loss,\n",
    "        \"train_accuracy\": train_accuracy,\n",
    "        \"validation_accuracy\": validation_accuracy,\n",
    "    }\n",
    "\n",
    "    wandb.log(log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2026-01-29T16:02:36.541356Z",
     "iopub.status.busy": "2026-01-29T16:02:36.541111Z",
     "iopub.status.idle": "2026-01-29T16:02:37.185075Z",
     "shell.execute_reply": "2026-01-29T16:02:37.184470Z",
     "shell.execute_reply.started": "2026-01-29T16:02:36.541330Z"
    },
    "id": "bePQ4KwpgVEl",
    "outputId": "ac5a3428-07dd-4cc0-cd34-37400216e847",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# logging weights\n",
    "model_filename = \"certified_model.pt\"\n",
    "torch.save(certified_model.state_dict(), model_filename)\n",
    "artifact = wandb.Artifact(\"model\", type=\"model\")\n",
    "artifact.add_file(f\"/kaggle/working/{model_filename}\")\n",
    "wandb.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "execution": {
     "iopub.execute_input": "2026-01-29T16:02:37.186130Z",
     "iopub.status.busy": "2026-01-29T16:02:37.185867Z",
     "iopub.status.idle": "2026-01-29T16:02:38.921051Z",
     "shell.execute_reply": "2026-01-29T16:02:38.920372Z",
     "shell.execute_reply.started": "2026-01-29T16:02:37.186108Z"
    },
    "id": "MHOA3w3BgVu1",
    "outputId": "fe4436b9-6492-4b06-847b-86675f22dc06",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Certified Contrastive Model\n",
    "\n",
    "The steps are:\n",
    "1. train the encoder using bounds propagation: the lower bound is passed to the supervised contrastive loss;\n",
    "2. train the classifier using the cross entropy loss with the lower bounds obtained by the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2026-01-29T16:06:46.347746Z",
     "iopub.status.busy": "2026-01-29T16:06:46.347004Z",
     "iopub.status.idle": "2026-01-29T16:06:46.404352Z",
     "shell.execute_reply": "2026-01-29T16:06:46.403766Z",
     "shell.execute_reply.started": "2026-01-29T16:06:46.347718Z"
    },
    "id": "ke50iicnggue",
    "outputId": "5f119d39-d7a7-46d8-ff51-c27cb1f25fc3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Certified Contrastive Model - Encoder\n",
    "\n",
    "epsilon = 2/255  # image perturbation\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 128\n",
    "learning_rate = 0.001\n",
    "sup_con_loss = SupConLoss()\n",
    "certified_contrastive_encoder = Encoder(proj_dim=PROJ_DIM,).to(DEVICE)\n",
    "certified_contrastive_encoder = BoundedModule(certified_contrastive_encoder, torch.empty(2, 3, 32, 32))\n",
    "certified_contrastive_encoder_optimizer = optim.Adam(certified_contrastive_encoder.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "execution": {
     "iopub.execute_input": "2026-01-29T16:06:50.539135Z",
     "iopub.status.busy": "2026-01-29T16:06:50.538547Z",
     "iopub.status.idle": "2026-01-29T16:06:56.776044Z",
     "shell.execute_reply": "2026-01-29T16:06:56.775253Z",
     "shell.execute_reply.started": "2026-01-29T16:06:50.539110Z"
    },
    "id": "_dSLNYQlhR8f",
    "outputId": "6661844e-33ee-4403-da50-c8c9b2faa733",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"Cnn-Verification\",\n",
    "    name=f\"Certified Contrastive Encoder - {AUGMENTATION_LABEL} Augmentation\",\n",
    "    config={\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"projection_dimension\": PROJ_DIM,\n",
    "        \"loss\": \"Supervised Contrastive Loss\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 601
    },
    "execution": {
     "iopub.execute_input": "2026-01-29T16:07:00.362653Z",
     "iopub.status.busy": "2026-01-29T16:07:00.362016Z"
    },
    "id": "HAIyk92FhB_4",
    "outputId": "a09b02c0-711e-47d2-ecfb-fcd665dda9ee",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train loop\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=ResourceWarning)\n",
    "\n",
    "ptb = PerturbationLpNorm(norm=float('inf'), eps=epsilon)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    certified_contrastive_encoder.train()\n",
    "    train_loss = 0\n",
    "    train_accuracy = 0\n",
    "    for _, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        images_bounded = BoundedTensor(images, ptb)\n",
    "\n",
    "        certified_contrastive_encoder_optimizer.zero_grad()\n",
    "        lb, ub = certified_contrastive_encoder.compute_bounds(x=(images_bounded,), method=\"CROWN-IBP\")\n",
    "        loss = sup_con_loss(lb, labels)\n",
    "        loss.backward()\n",
    "        certified_contrastive_encoder_optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_accuracy += (torch.argmax(lb, dim=1) == labels).sum().item() / len(labels)\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    certified_contrastive_encoder.eval()\n",
    "    with torch.no_grad():\n",
    "        validation_loss = 0\n",
    "        validation_accuracy = 0\n",
    "        for images, labels in validation_loader:\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            images_bounded = BoundedTensor(images, ptb)\n",
    "\n",
    "            lb, ub = certified_contrastive_encoder.compute_bounds(x=(images_bounded,), method=\"CROWN-IBP\")\n",
    "            loss = sup_con_loss(lb, labels)\n",
    "\n",
    "            validation_loss += loss.item()\n",
    "            validation_accuracy += (torch.argmax(lb, dim=1) == labels).sum().item() / len(labels)\n",
    "\n",
    "    validation_loss /= len(validation_loader)\n",
    "\n",
    "    print(f\"> Epoch {epoch+1}/{EPOCHS}\")\n",
    "    print(f\"  Training loss      : {train_loss:.4f}\")\n",
    "    print(f\"  Validation loss    : {validation_loss:.4f}\")\n",
    "\n",
    "    log = {\n",
    "        \"train_loss\": train_loss,\n",
    "        \"validation_loss\": validation_loss,\n",
    "    }\n",
    "\n",
    "    wandb.log(log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "slKneyb9iBAI",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# logging weights\n",
    "model_filename = \"certified_contrastive_encoder.pt\"\n",
    "torch.save(certified_contrastive_encoder.state_dict(), model_filename)\n",
    "artifact = wandb.Artifact(\"model\", type=\"model\")\n",
    "artifact.add_file(f\"/kaggle/working/{model_filename}\")\n",
    "wandb.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "Wr-f7TACj1wL",
    "outputId": "3ac5fb71-e400-428f-db74-1697dc9bb968",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1N_3PpKRj-w8",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Certified Contrastive Model - Classifier\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "learning_rate = 0.001\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "certified_contrastive_classifier = LinearClassifier(in_dim=PROJ_DIM).to(DEVICE)\n",
    "certified_contrastive_classifier_optimizer = optim.Adam(certified_contrastive_classifier.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"Cnn-Verification\",\n",
    "    name=f\"Certified Contrastive Classifier - {AUGMENTATION_LABEL} Augmentation\",\n",
    "    id=\"cy5rejql\",\n",
    "    resume=\"allow\",\n",
    "    config={\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"projection_dimension\": PROJ_DIM,\n",
    "        \"loss\": \"CrossEntropy Loss\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_certfied_embeddings(certified_contrastive_encoder:nn.Module, epsilon:float) -> Callable[[torch.Tensor, torch.Tensor], tuple[torch.Tensor, torch.Tensor]]:\n",
    "    # used to get the perturbed embeddings from the encoder\n",
    "    \n",
    "    def main(images:torch.Tensor, labels:torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        certified_contrastive_encoder.eval()\n",
    "        ptb = PerturbationLpNorm(norm=float('inf'), eps=epsilon)\n",
    "        images_bounded = BoundedTensor(images, ptb)\n",
    "        with torch.no_grad():\n",
    "            certified_contrastive_encoder_optimizer.zero_grad()\n",
    "            embeddings, _ = certified_contrastive_encoder.compute_bounds(x=(images_bounded,), method=\"CROWN-IBP\")\n",
    "        return embeddings, labels\n",
    "        \n",
    "    return main\n",
    "\n",
    "certified_contrastive_classifier = train(\n",
    "    certified_contrastive_classifier,\n",
    "    train_loader,\n",
    "    validation_loader,\n",
    "    certified_contrastive_classifier_optimizer,\n",
    "    cross_entropy_loss,\n",
    "    EPOCHS,\n",
    "    DEVICE,\n",
    "    middleware=get_certfied_embeddings(certified_contrastive_encoder, epsilon),\n",
    "    wandb_logging=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dliMaGymk7O2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# logging weights\n",
    "model_filename = \"certified_contrastive_classifier.pt\"\n",
    "torch.save(certified_contrastive_classifier.state_dict(), model_filename)\n",
    "artifact = wandb.Artifact(\"model\", type=\"model\")\n",
    "artifact.add_file(f\"/kaggle/working/{model_filename}\")\n",
    "wandb.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FBiTqCftk_CJ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3RAQmsuZPDb"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JP_Lw7j0ZPDb"
   },
   "source": [
    "# Testing\n",
    "\n",
    "Since some models are trained with augmented data, adversarial examples and intervals propagation, let's compute the accuracy on the original CIFAR10 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import multiprocessing\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from auto_LiRPA import BoundedModule, BoundedTensor, PerturbationLpNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "multiprocessing.set_start_method('spawn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KKYh2gTGZPDb",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# loading all the models\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "augmentation_path = \"/kaggle/input/cnnrobust/pytorch/nopooling_models/5/augmentation\"\n",
    "no_augmentation_path = \"/kaggle/input/cnnrobust/pytorch/nopooling_models/5/no_augmentation\"\n",
    "\n",
    "# loading no certified models\n",
    "models_weights = [\n",
    "    torch.load(f\"{augmentation_path}/normal_model.pt\"),\n",
    "    torch.load(f\"{augmentation_path}/contrastive_model.pt\"),\n",
    "    torch.load(f\"{augmentation_path}/adversarial_model.pt\"),\n",
    "    torch.load(f\"{augmentation_path}/adversarial_contrastive_model.pt\"),\n",
    "    torch.load(f\"{no_augmentation_path}/normal_model.pt\"),\n",
    "    torch.load(f\"{no_augmentation_path}/contrastive_model.pt\"),\n",
    "    torch.load(f\"{no_augmentation_path}/adversarial_model.pt\"),\n",
    "    torch.load(f\"{no_augmentation_path}/adversarial_contrastive_model.pt\"),\n",
    "]\n",
    "\n",
    "models = []\n",
    "\n",
    "for weights in models_weights:\n",
    "    model = CNNCrown()\n",
    "    model.load_state_dict(weights)\n",
    "    models.append(model)\n",
    "\n",
    "# -- loading certified models by hand\n",
    "# with augmentation\n",
    "\n",
    "certified_encoder = BoundedModule(Encoder(), torch.empty(2, 3, 32, 32))\n",
    "certified_encoder.load_state_dict(torch.load(f\"{augmentation_path}/certified_contrastive_encoder.pt\"))\n",
    "certified_classifier = LinearClassifier()\n",
    "certified_classifier.load_state_dict(torch.load(f\"{augmentation_path}/certified_contrastive_classifier.pt\"))\n",
    "certified_contrastive_model = CNNCrown()\n",
    "certified_contrastive_model.encoder = certified_encoder\n",
    "certified_contrastive_model.classifier = certified_classifier\n",
    "models[4:4] = [certified_contrastive_model]    # the models trained using augmentation are in the first part of the list\n",
    "\n",
    "certified_model = BoundedModule(CNNCrown(), torch.empty(2, 3, 32, 32))\n",
    "certified_model.load_state_dict(torch.load(f\"{augmentation_path}/certified_model.pt\"))\n",
    "models[4:4] = [certified_model]    # the models trained using augmentation are in the first part of the list\n",
    "\n",
    "# no augmentation\n",
    "certified_model = BoundedModule(CNNCrown(), torch.empty(2, 3, 32, 32))\n",
    "certified_model.load_state_dict(torch.load(f\"{no_augmentation_path}/certified_model.pt\"))\n",
    "models.append(certified_model)    # the models trained using no augmentation are in the last part of the list\n",
    "\n",
    "certified_encoder = BoundedModule(Encoder(), torch.empty(2, 3, 32, 32))\n",
    "certified_encoder.load_state_dict(torch.load(f\"{no_augmentation_path}/certified_contrastive_encoder.pt\"))\n",
    "certified_classifier = LinearClassifier()\n",
    "certified_classifier.load_state_dict(torch.load(f\"{no_augmentation_path}/certified_contrastive_classifier.pt\"))\n",
    "certified_contrastive_model = CNNCrown()\n",
    "certified_contrastive_model.encoder = certified_encoder\n",
    "certified_contrastive_model.classifier = certified_classifier\n",
    "models.append(certified_contrastive_model)    # the models trained using no augmentation are in the last part of the list\n",
    "\n",
    "\n",
    "models_name = [\"Normal Model\", \"Contrastive Model\", \"Adversarial Model\", \"Adversarial Contrastive\", \"Certified\", \"Certified Contrastive\"] * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# loading dataset - no augmentation used\n",
    "torch.manual_seed(42)\n",
    "BATCH_SIZE = 2048\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "dataset = datasets.CIFAR10(root=\"data\", train=True, download=True, transform=transform)\n",
    "\n",
    "train_ratio, validation_ratio = 0.8, 0.2\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(train_ratio * dataset_size)\n",
    "validation_size = dataset_size - train_size\n",
    "\n",
    "train_dataset, validation_dataset = random_split(dataset, [train_size, validation_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8)\n",
    "\n",
    "test_data = datasets.CIFAR10(root=\"data\", train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"> Accuracy\")\n",
    "for model_id, (model, model_name) in enumerate(zip(models, models_name)):\n",
    "    train_accuracy, test_accuracy = test(model, train_loader, test_loader, DEVICE)\n",
    "    \n",
    "    if model_id == 0:\n",
    "        print(\"\\t- Augmentation\")\n",
    "    if model_id == 6:\n",
    "        print(\"\\t- No Augmentation\")\n",
    "\n",
    "    print(f\"\\t\\t- {model_name}: {train_accuracy:.2f}% -> {test_accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelId": 569571,
     "modelInstanceId": 558930,
     "sourceId": 735340,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
